{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2ccfe-b280-46fd-b6a3-e685b2551032",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": [
    "## Create DB, Schema, WH\n",
    "if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530cbbf-77d0-4b5b-a21f-aba1e695990e",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "-- CREATE DATABASE If NOT EXISTS MASS_SEARCH;\n",
    "-- USE DATABASE MASS_SEARCH;\n",
    "-- CREATE SCHEMA If NOT EXISTS DATA;\n",
    "-- USE SCHEMA DATA;\n",
    "\n",
    "-- CREATE OR REPLACE WAREHOUSE CHAT_WH\n",
    "-- WITH WAREHOUSE_SIZE = 'MEDIUM'\n",
    "-- AUTO_SUSPEND = 120\n",
    "-- AUTO_RESUME = TRUE\n",
    "-- INITIALLY_SUSPENDED = TRUE;\n",
    "\n",
    "-- USE WAREHOUSE cortex_search_wh;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8ef19-5673-44f4-a211-0064f3f40f9e",
   "metadata": {
    "collapsed": false,
    "name": "cell5"
   },
   "source": [
    "## SLIT FUNCTION - UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4e93c-7466-49d2-9bf1-158e84b11e1e",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "-- CREATE FUNCTION TO SPLIT PDFs\n",
    "create or replace function <DB_NAME>.<SCHEMA_NAME>.text_chunker(pdf_text string)\n",
    "returns table (chunk_order integer, chunk varchar)\n",
    "language python\n",
    "runtime_version = '3.9'\n",
    "handler = 'text_chunker'\n",
    "packages = ('snowflake-snowpark-python', 'langchain')\n",
    "as\n",
    "$$\n",
    "from snowflake.snowpark.types import StringType, StructField, StructType\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "\n",
    "class text_chunker:\n",
    "\n",
    "    def process(self, pdf_text: str):\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = 1512, #Adjust this as you see fit\n",
    "            chunk_overlap  = 256, #This let's text have some form of overlap. Useful for keeping chunks contextual\n",
    "            length_function = len\n",
    "        )\n",
    "    \n",
    "        chunks = text_splitter.split_text(pdf_text)\n",
    "        df = pd.DataFrame(chunks, columns=['chunks'])\n",
    "        \n",
    "        yield from df.itertuples(index=True, name=None)\n",
    "$$;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076468d-33e0-4a3b-827e-87275e515436",
   "metadata": {
    "collapsed": false,
    "name": "cell6"
   },
   "source": [
    "## CREATE STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac00ea4-727a-40c1-9b9b-02bccc21354a",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "-- CREATE STAGE TO HOLD DOCS \n",
    "create or replace stage <DB_NAME>.<SCHEMA_NAME>.<STAGE_NAME> ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3048fbe5-5121-4527-8616-1427d0dd1db4",
   "metadata": {
    "collapsed": false,
    "name": "cell8"
   },
   "source": [
    "## UPLOAD DOCS\n",
    "Before moving on to the next step, upload the docs to stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdae41f-c8da-43e0-b747-8e532d37f174",
   "metadata": {
    "language": "sql",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "-- CHECK TO SEE IF LOADING IS SUCCESSFUL\n",
    "ls @<DB_NAME>.<SCHEMA_NAME>.<STAGE_NAME>; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac79f3-f171-462c-aed2-a5f30daf8314",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "## CREATE TABLE FOR THE CHUNKS TO BE STORED FOR EACH DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a7b92f-b41c-4923-9241-96aa58580e46",
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "-- TABLE FOR THE CHUNKS TO BE STORED FOR EACH PDF\n",
    "\n",
    "create or replace TABLE <DB_NAME>.<SCHEMA_NAME>.<TABLE_NAME> ( \n",
    "    RELATIVE_PATH VARCHAR(16777216), -- Relative path to the PDF file\n",
    "    SIZE NUMBER(38,0), -- Size of the PDF\n",
    "    FILE_URL VARCHAR(16777216), -- URL for the PDF\n",
    "    SCOPED_FILE_URL VARCHAR(16777216), -- Scoped url (you can choose which one to keep depending on your use case)\n",
    "    CHUNK_ORDER INTEGER, -- Order of the chunk in the original document\n",
    "    CHUNK VARCHAR(16777216) -- Piece of text\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77a050c-834f-44ee-ae03-151f690f8196",
   "metadata": {
    "collapsed": false,
    "name": "cell12"
   },
   "source": [
    "## PARSE & CHUNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95f8e1-661d-4d02-8518-d5bc4502c0e7",
   "metadata": {
    "language": "sql",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "-- USE CORTEX PARSE_DOCUMENT TO READ AND USE FUNCTION CREATED TO CHUNK\n",
    "insert into <DB_NAME>.<SCHEMA_NAME>.<TABLE_NAME> (relative_path, size, file_url,\n",
    "                            scoped_file_url, chunk_order, chunk)\n",
    "\n",
    "    select relative_path, \n",
    "            size,\n",
    "            file_url, \n",
    "            build_scoped_file_url(@<DB_NAME>.<SCHEMA_NAME>.<STAGE_NAME>, relative_path) as scoped_file_url,\n",
    "            func.chunk_order as chunk_order,\n",
    "            func.chunk as chunk\n",
    "    from \n",
    "        directory(@<DB_NAME>.<SCHEMA_NAME>.<STAGE_NAME>),\n",
    "        TABLE(text_chunker (TO_VARCHAR(SNOWFLAKE.CORTEX.PARSE_DOCUMENT(@<DB_NAME>.<SCHEMA_NAME>.<STAGE_NAME>, relative_path, {'mode': 'LAYOUT'})))) as func;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95127467-2a48-4fff-b25e-dedf10591667",
   "metadata": {
    "language": "sql",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "-- CHECK CHUNKS TABLE\n",
    "select *\n",
    "from  @<DB_NAME>.<SCHEMA_NAME>.<TABLE_NAME>;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e8d01-1ac9-464d-adf2-5eef0741feaf",
   "metadata": {
    "collapsed": false,
    "name": "cell15"
   },
   "source": [
    "## CREATE CORTEX SEARCH SERVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5ddd91-64ac-4320-ae35-abcdb12794bb",
   "metadata": {
    "language": "sql",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "-- CREATE CORTEX SEARCH SERVICE\n",
    "create or replace CORTEX SEARCH SERVICE @<DB_NAME>.<SCHEMA_NAME>.<SEARCH_SERVICE_NAME>\n",
    "ON chunk\n",
    "ATTRIBUTES RELATIVE_PATH, CHUNK_ORDER\n",
    "warehouse = CHAT_WH\n",
    "TARGET_LAG = '365 DAYS'\n",
    "as (\n",
    "    select chunk,\n",
    "        relative_path,\n",
    "        chunk_order,\n",
    "        file_url\n",
    "    from @<DB_NAME>.<SCHEMA_NAME>.<TABLE_NAME>\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d1182-f897-4de8-a3d8-e1abf64d20fc",
   "metadata": {
    "language": "sql",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "-- Query the service using SEARCH_PREVIEW\n",
    "SELECT SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n",
    "    '<DB_NAME>.<SCHEMA_NAME>.<SEARCH_SERVICE_NAME>',\n",
    "    '{\n",
    "        \"query\": \"<QUERY>\",\n",
    "        \"columns\": [\"chunk\"],\n",
    "        \"limit\": 5\n",
    "    }'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7018e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Dropping Cortex Search service\n",
    "--DROP CORTEX SEARCH SERVICE IF EXISTS <DB_NAME>.<SCHEMA_NAME>.<SEARCH_SERVICE_NAME>;\n",
    "\n",
    "-- Dropping document chunks table\n",
    "--DROP TABLE IF EXISTS <DB_NAME>.<SCHEMA_NAME>.<TABLE_NAME>;\n",
    "\n",
    "-- Dropping stage with PDF files\n",
    "--DROP STAGE IF EXISTS <DB_NAME>.<SCHEMA_NAME>.<STAGE_NAME>;\n",
    "\n",
    "\n",
    "-- drop schema and database\n",
    "-- DROP SCHEMA IF EXISTS <DB_NAME>.<SCHEMA_NAME>;\n",
    "-- DROP DATABASE IF EXISTS <DB_NAME>;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "taj.akmal@snowflake.com",
   "authorId": "1935988576203",
   "authorName": "TAJA",
   "lastEditTime": 1742491008823,
   "notebookId": "cmhjjchnydlurtrz43ra",
   "sessionId": "cdd4671b-556c-46e9-babf-3a96672f6a8c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
